---
title: Assignment 3
author: Oliver Daniel
subtitle: |
    | for INF 412 (J. Wang)
    | March 26, 2023
bibliography: '.bib'
link-citations: true
output:
  bookdown::html_document2:
    number_sections: false
    fig_caption: true
  bookdown::pdf_document2:
    number_sections: false
    fig_caption: true
    toc: true
    toc_depth: 4
header-includes:
  - \usepackage{float}
  - \floatplacement{figure}{H}
  - \floatplacement{table}{H}

---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(
    echo = FALSE,
    message = FALSE,
    warning = FALSE,
    cache = TRUE,
    out.width = "100%",
    out.align = "center"
)
```

```{r libraries}
library(tidyverse)
library(patchwork)
```

```{r util}
fmt <- function(n, is_double = FALSE) {
    nsmall <- `if`(
        is_double, 2, 0
    )
    format(round(n, digits = nsmall), nsmall = nsmall, big.mark = ",")
}

tfmt <- function(s) {
    strwrap(s, 50)
}
```

\tableofcontents

## Poisson Regression
### 1. Dataset
The dataset, *Bike Share Toronto Ridership Data*^[Contains information licensed under the Open Government Licence – Toronto.], was accessed through the Open Data Toronto portal. In particular, the 2021 ridership data were selected due to an anomaly in the more recent 2022 set's file structure.



```{r prepare-data}
source("scripts/fetch.r")

df <- data.all |>
    mutate(
        month = lubridate::month(start, label = TRUE),
        hour = lubridate::hour(start) |> as.factor()
    )

option_grid <- expand.grid(
    month = month.abb,
    hour = as.character(0:23),
    user_type = c("Annual Member", "Casual Member")
) |> as_tibble()
```

The content of the dataset is a zipped series of CSVs, one per calendar month of 2021, with each row representing a single completed journey of a Toronto Bike Share unit (henceforth "city bike"), from its release at a particular station, at a particular time, to its deposit at another – potentially different – station, at a particular time. Also included are UUIDs for the trip and city bike unit. In total, there are `r fmt(nrow(data.all))` unique trip events across the 12 months.

### Columns
Of the 10 columns included with the data, only two were retained:

* **Start Time**: A timestamp representing the date and time at which the trip began (i.e., when the city bike was released from its holder).
* **User Type**: One of `Annual Member` or `Casual Member`, representing whether the purchaser of the trip was an annual pass bearer, or merely paid for the singular trip.

## 2. Variables
### Independent Variables
**Start Time** was further divided into two columns, representing each timestamp's constituent date and time, respectively. To that end, for the purposes of this paper, two particular facets will be of focus as two of our three independent variables: the **month** in which the trip took place, and the **hour** of day during which the trip started. As expected, **month** and **hour** are  ordered categorical variables, enumerating from 1 (`Jan`) to 12 (`Dec`), and from `0` (midnight) to `23` (11 PM), respectively.

The third variable shall be **User Type**, as previously described.

## 3. Dependent Variable & Regression Type
For the purposes of this assignment, a **Poisson regression** shall be used, to predict the total number of unique city bike trips that might be expected at the intersection of the three independent variables. For example, the regression model should be able to predict how many total trips will be started by pass-bearing members at noon in July. With this level of specificity, we are also able to get estimates over ranges by summing predictions over the ranges of elided variables; e.g., predicting all trips by pass-bearing in July, regardless of time of day, by summing all predicted trips from midnight to 11pm.

Poissonian models are useful for predicting the total **count of independent events** that might occur as of a particular point in time. We assume that all bike trips taken throughout Toronto are independent of one another, or at least insignificantly co-dependent. Here, our dependent variable is indeed count data – a total number of trips taken – so a Poisson regression is a useful model for our prediction.

## 4. Variable Distributions

```{r distribCount, fig.cap="Distribution of 2021 City Bike rentals by week"}
df |>
    ggplot(aes(x = start)) +
    geom_histogram(bins = 52) +
    scale_x_datetime(
        date_labels = "%b",
        date_breaks = "1 month"
    ) +
    scale_y_continuous(labels = scales::comma) +
    labs(
        x = "Rental date",
        y = "Quantity"
    )
```

```{r distribUserType, fig.cap="Distribution of user type in 2021 City Bike rentals"}
df |>
    ggplot(aes(x = user_type)) +
    geom_histogram(stat = "count") +
    labs(
        x = NULL,
        y = "Quantity"
    )
```

## 5. Correlational Diagrams

The correlation of our three independent variables to city bike count can be succinctly visualized as follows:

```{r distribStartTime, fig.cap="Distribution of 2021 City Bike rentals by month, time of day, and member type"}
df |>
    count(month, hour, user_type) |>
    ggplot(aes(x = hour, y = n)) +
    geom_col() +
    scale_x_discrete(breaks = seq(0, 24, 2)) +
    scale_y_continuous(breaks = c(0, 3E4), guide = guide_axis(check.overlap = TRUE)) +
    facet_grid(month ~ user_type) +
    labs(
        x = "Hour of day at rental",
        y = "Quantity"
    )
```

A number of interesting patterns can be seen here. With both types of members, there is a peak in monthly use in the warmer months (May through August), tapering off through the remainder of the year. Although annual members account for many more rentals in most of the colder months, casual members surprisingly dominate in October and November.

```{r diffCasualAnnual}
df |>
    filter(user_type == "Casual Member") |>
    count(month) |>
    inner_join(
        df |>
            filter(user_type == "Annual Member") |>
            count(month),
        by = "month",
        suffix = c(".casual", ".annual")
    ) |>
    mutate(
        diff = n.casual - n.annual
    ) |>
    knitr::kable(
        col.names = c(
            "Month",
            "Casual rentals",
            "Annual rentals",
            "Difference"
        ),
        format.args = list(
            big.mark = ","
        ),
        caption = "Monthly comparison of casual vs. annual members in counts of bike rentals"
    )
```

See Figure \@ref(fig:heatmaps) in the [Appendix] for a density map correlating start month and hour among casual and annual membership.

## 6. Pre-regression analysis
Looking at the above figures, it seems visually that the date and time of rental bear a *non-*linear relationship to the total number of bike rentals in any given month. Indeed, Figure \@ref(fig:distribCount) seems to exhibit a rough normal curve over the year with a peak in the summer months, and Figure \@ref(fig:distribStartTime) shows a similar, 'normal-ish' curve over the time of day, regardless of month.

However, the month and user type appear to apply something of a static multiplier to each of these distributions. Although they are not likely to be perfectly linear across their enumeration, the impact that, say, being a casual vs. annual member has on total rentals in April appears more linear.

\newpage
## 7. Regression analysis
```{r echo=TRUE}
combined_model <- df |>
    mutate(month = factor(month, ordered = FALSE)) |>
    count(month, hour, user_type) |>
    glm(
        formula = n ~ month + hour + user_type,
        family = poisson
    )

summary(combined_model)
```

```{r}
predicted_rentals <- function(month, hour, user_type) {
    predict(
        combined_model,
        newdata = data.frame(
            month = month,
            hour = hour,
            user_type = user_type
        ),
        type = "response"
    )
}
```

\newpage
## 8. Coefficient exponentiation

```{r echo=TRUE}
exponentiated_coefficients <- exp(summary(combined_model)$coefficients)
exponentiated_coefficients |> knitr::kable()
```

```{r}
intercept_est <- fmt(exponentiated_coefficients["(Intercept)", "Estimate"], FALSE)
casual_est <- fmt(exponentiated_coefficients["user_typeCasual Member", "Estimate"], TRUE)
eqn <- paste(intercept_est, "\\times", casual_est, "\\approx", round(predicted_rentals("Jan", "0", "Casual Member")))
```

## Analysis
### 9. Coefficients
Our first exponentiated coefficient is the intercept rate, which in this case is the expected total city bike trips taken by annual members, beginning at midnight, through the month of January. In other words, our models predicts that `r intercept_est` annual members would rent a city bike between midnight and 1 AM. The remaining coefficients represent a ratio by which, if all other values are held constant, this rate would be expected to change. For example, the coefficient for casual member rentals is `r casual_est`, so we would expect something like $`r eqn`$ total rentals from casual members from midnight to 1AM in January. These coefficients can be multiplied together, one per categorical independent variable, to specify additional parameters.

The patterns of these coefficients across different levels of each variable resemble the visual properties of Figure \@ref(fig:distribStartTime). For example, `user_typeCasual Member` being a negative logarithmic value makes sense, as there appeared to be something of a general diminishing effect on rentals regardless of time of day or month. The highest multiplicative values for month are found in the summer months (June – September), and February – often the coldest month – has a *chilling* effect on rentals. As for hours, the late afternoon and early evening (4PM – 7PM) are correlated with larger increases in rental rate, whereas the small hours of the morning (1AM – 5AM) are correlated with large reductions in rate, before more than doubling several times over into 8AM.

### 10. Significance
The summary table for the regression in [7. Regression Analysis] calculates that the *p*-value for every row is less than $2 \times {10}^{-16}$, meaning that each value of each variable is a highly significant predictor of rental rates. Calculating a simple McFadden's pseudo-$R^2$ for a rough approximation of goodness of fit yields:

```{r, echo=TRUE}
mcfadden_r2 <- with(summary(combined_model), 1 - deviance / null.deviance)
mcfadden_r2
```

So, our model is able to account for roughly `r round(mcfadden_r2 * 100)`% of variance in city bike rentals over hours, months, and types of renting members.


## Extra Credit: Predictions

Because all of our independent variables are categorical and finite, we can actually use `expand.grid` to calculate model predictions for all `r nrow(option_grid)` possible values that it can account for. Figure \@ref(fig:heatmaps) in the [Appendix] shows density maps for bike rentals in the real data, as well as our Poissonian model. The two are visually similar, but the model predictions look more 'de-noised' or regular compared to the Open Data Toronto plot. But, it misses some interesting details, such as the higher density of casual member rentals in October compared to annual members.

\newpage
## Appendix
```{r}
predictions <- option_grid |>
    mutate(
        predicted = predicted_rentals(month, hour, user_type)
    )
```
```{r heatmaps, fig.cap="Comparison of real vs. predicted data for 2021 City Bike rentals"}
(df |>
    count(hour, month, user_type) |>
    ggplot(aes(x = hour, y = fct_rev(month), fill = n)) +
    geom_tile() +
    scale_fill_viridis_c(option = "magma") +
    scale_x_discrete(breaks = seq(0, 24, 2)) +
    facet_grid(~user_type, scales = "free") +
    labs(
        title = "Open Data Toronto data",
        x = "Hour of day",
        y = "Month",
    )
) /
    (predictions |>
        ggplot(aes(x = hour, y = fct_rev(month), fill = predicted)) +
        geom_tile() +
        scale_fill_viridis_c(option = "magma") +
        scale_x_discrete(breaks = seq(0, 24, 2)) +
        facet_grid(~user_type, scales = "free") +
        labs(
            title = "Poisson model predictions",
            x = "Hour of day",
            y = "Month"
        )
    ) +
    plot_annotation()
```